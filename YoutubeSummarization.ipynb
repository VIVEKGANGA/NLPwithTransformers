{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwxQQFEnGO8m",
        "outputId": "9a2c3b82-9033-40a5-9456-eecc573d7847"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -q -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain import OpenAI, LLMChain\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.chains.mapreduce import MapReduceChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores.faiss import FAISS\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "import textwrap \n",
        "import whisper\n",
        "import yt_dlp\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Accesing API keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xlc9F838GeDe"
      },
      "outputs": [],
      "source": [
        "dotenv_path = '.env'\n",
        "load_dotenv(dotenv_path)\n",
        "os.environ['OPENAI_API_KEY'] = os.getenv(\"OPENAI_API_KEY\")\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = os.getenv(\"HUGGINGFACE_API_TOKEN\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_7f4F6iG48p",
        "outputId": "2fa0f388-44b7-48ba-b3ab-2522deb4c652"
      },
      "outputs": [],
      "source": [
        "def download_mp4_from_youtube(url):\n",
        "  filename=\"yt_audio.mp4\"\n",
        "  ydl_opts = {\n",
        "        'format': 'bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]',\n",
        "        'outtmpl': filename,\n",
        "        'quiet': True,\n",
        "    }\n",
        "  with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "    result = ydl.extract_info(url, download=True)\n",
        "\n",
        "def transcribe_video(audio_file):\n",
        "  model = whisper.load_model(\"base\")\n",
        "  result = model.transcribe(\"yt_audio.mp4\")\n",
        "  with open(\"transcibed_file.txt\",\"w\") as file:\n",
        "    file.write(result['text'])\n",
        "  \n",
        "\n",
        "url = \"https://www.youtube.com/watch?v=LkhQQOzfczc\"\n",
        "download_mp4_from_youtube(url)\n",
        "transcribe_video(\"yt_audio.mp4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Calling OpenAI LLM and Embedding models "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3Mn8yOpHoaA",
        "outputId": "82f5be62-673a-423e-f561-6db87cbcbc07"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████| 139M/139M [00:01<00:00, 79.9MiB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Okay, so one of the biggest challenges as you're building LLM apps is both testing it yourself and getting it in the hands of users to get feedback. So I think there are sort of two main things that you want to think about here. You want to think about building quick UIs that people can test things in, which I'm going to talk about in this video. Then you also want to think about automated testing, which perhaps we saved through a future video. So the first way of doing this is you want to basically get your idea, your app, your product, your model into the hands of end users as quickly as possible. This allows you very quickly to just get feedback on what people like, what people don't like. Often when you're coding and making something for the first time, unless you're making it for yourself, and often even when you are making it for yourself, when you actually start to use it, you realize that a lot of the assumptions you had were perhaps totally wrong. So this is where you're looking for a good, quick framework that can basically allow you to build a UI quickly to test something out. So up until now we've had streamlets, we've had radio. And now a group of coders at Google have released this framework called mess up. Now, mess up is not an official Google product. This is basically a product that a number of the coders have been working on in their 20% time and probably on their spare time as well. The whole idea of this is to be able to build a web UIs quickly with Python and really is aimed at engineers who don't have a lot of front end skills. If you think back to streamlet, streamlet has become very popular. Snowflake bought streamlet for a lot of money. Early on, streamlet was actually used inside of Google. Google is one of the early adopters of streamlet. Now, my guess is now that another big company owns it. They probably don't want to use it as much internally. And also it does seem that streamlet has gone in a different direction a little bit since Snowflake acquired them. And then on the other side, you've got radio that was bought by Hing face. It makes sense for people inside Google to want to have their own thing that they can control. And this is where mess up comes in. Now they've open sourced this so you can use this straight away. And it's still a bit off being a version one in here. They've got it to a version 0.8 in here. But let's have a look at it. And then I'll, what I'll do is go through the key elements of it. And then we will also look at building a chatbot with Langchain with Grok, with some memory, how you would actually do this to be able to use this. OK, so in their blog posts, they basically talk about why they're doing this, the whole sort of idea that a lot of developers just don't have the front end skills for doing this. The other thing is they wanted something that is very easy and quick to put together. And I think this is where this becomes really interesting. The whole idea of having something that's already got components that you can just basically use as well as make your sort of own low level kind of thing in here. So if we come in and have a look at the components that they've got already, we've got really high level sort of components like this sort of chat idea here, where we can basically just type in something and we can see that it can respond back. Now how does that actually work? Here's the code for it. You can see there's not a lot of code in there. Basically you're just setting up a page, et cetera. And then you've got a chat implementation and you give it a function that will determine what to do with that. We'll look at that in a moment when we look at the Lange in example. And you should notice that in here, one of the key things in here is that this is actually building a flask app on the fly. So you've got all these elements that are basically being built for you in the back end and just giving you a nice, simple UI at the front. So they've got some high level components. So we've got chat, we've got text to text examples, we've got text image examples. If you wanted to make some kind of image manipulation tool or something like that, you've also then got lower level stuff of where you've got boxes. You can see that if you want to go and design your own thing a bit more, you've got the sidebar navigation, you've got text, you've got marked down, you've got a whole bunch of these different things. You've obviously got things like buttons, text inputs, all these sorts of things that you can use. And on top of this, you've also got a bunch of demos that they've done. So you can see in here, if we look at like the LLM playground, we can see that we've got something now where we can select the model, we can select the region, we can play around with the temperature, we can do it, set a whole bunch of different things in here. And we've got a full example of the code of how to do this. So they've got a number of these different demos that you can try things out. So if you wanted to do build something that's text to image, you can just come in here, take that code, reuse it. If you want to build sort of small elements, you can see exactly how they're doing that inside of these bigger demos in here. So I think in many ways, it's self explanatory of how you could put these things together. They've got a getting started colab. Let me just run this through one of the nice things that you can do in here is you can run all these in colab. So the girls are very special thing where you can do sort of me.colleb.run. So me, so it's just basically imported as me. You've got labs to basically give you the different components in here. And you can see that this will basically set up the sort of flask server running in the background. And then we can just add pages to this. So we can basically come in here. We can define a page with a decorator, you give it the name of the page. We just pass in a function for that page. You can see setting up a simple chat interface. It's pretty easy in here that we can basically just define that we're going to have a chat. We're going to basically set up that chat. We're going to have this mel. Dot chat. So that's basically where we're getting the components, putting that in. The main thing we need to define for the chat is the function that will handle the inputs from the UI and do something with it. Now you can see in this case, all that's going to do is basically just return hello and then whatever I ask it. So if I ask it, hi, how are you? You can see sure enough it just now returns hello, hi, how are you in here? Now we'll look at actually talking this up in a second to do some proper things. Overall though, you can set up, you know, multiple pages quite easily and quite quickly and put a bunch of things in them for when you want to test something out. So this makes it a great way just to get going, get something that you can test out quickly and build a very quick prototype, whether that's not just in colab, you can run it locally in VS code, etc. And go through it. All right, let's jump in and have a look at cooking this up with Langchain and Grohl just to make a simple chatbot with some memory in here. Okay, so in this example, I'm just going to walk through how you could set this up with Langchain and with Grohl to basically have a chat that's got memory that's retaining the memory as it goes through and that we could then basically test that out. Okay, I've got some packages that are brought in here. I brought in the Mesop package, I brought in the Langchain packages, the Langchain Grohl. I was going to do something with Dr. Goh. Maybe we'll do that in the future for this. So all right, I basically start off just by getting my GROC API key and then I'm going to set up the actual GROC chat part in here. So I'm going to basically set up a GROC LLM in here and then going to set up a prompt format for this. Okay, I've got my system prompt. Basically says you are a helpful assistant. Please be brief and concise into the point, answering as few words as possible, but still give the user the key info thereafter. Your name is Isabella and you are 28 years old, right? So that's coming from the system prompt here. The model that I'm actually going to use is Lama 370 billion on GROC. Then I've got a human input here where we're going to say use the conversation memory below to help answer the most recent query from the user. So then we're going to pass in a memory, which is going to just be sort of a list of everything that's been said. And then we're going to pass in a user query and then we're going to prompt it for getting the assistant message back in here. So all right, once I've got those, I basically just bring those in into the chat prompt template from messages. Set up a LENG chain expression language chain. So I've called this conversation chain because now I can basically evoke this by passing in a text message from the user. And then a string, which is going to be, or actually I've just changed this now to be a string that's going to be converted from a list of messages that we've had in here. So if I set up that chain, you can see that we asked it, how are you today? It replied, I'm doing great. Thanks. And then we've got all of these other data that we got back. All right. Now we don't need all of that that's coming back. What we need is the actual message. So that the message is going to be under response.content in there. All right. So next up, we want to build the actual me stop part. So we're going to bring in me stop as me. We're going to bring in the components. So me stop labs as male. We're going to run that to just get that started. And you can see, sure enough, it started now a flask server. Now you can see I've been running it already. But okay, we've got this going. If we were running this on a local machine, we could just click it and open it up straight away in collab will actually run sort of in line with the cells and stuff. All right. To make our chat is pretty simple. We've got the chat interface and we need to pass that into a function. So the function is going to be called transforming this case. And what it actually passes in is two things. It passes in what the user just pressed submit on. So like the last user query and it passes in a list of chat messages that it's calling history in here. So unfortunately, you can't simply sort of debug with this in collab. But you can see what I'm doing here is I'm going to basically set up a new string because what I want to do is up when I constantly to lane chain, I'm not going to pass in a list of chat messages. I'm going to convert it to a string. So what I'm going to do is basically just say going through this list of messages in history. For each one, we're just going to add the role and then the content of that. So the role will be, you know, user will be assistant kind of thing as we go through this. Then what we're going to do is we're going to run the conversation chain, which is going to call GROC. We're going to pass in the prompt that we got up here is going to go in here. And then we're going to pass in that history string that we've just made there. And this is going to give us a response back. And then really we just want the response dot content. So that's what we'll return back to the UI for this. Now, it will handle the state for this. It will handle the state of, you'll see now when I run it, it will handle the state of storing the different chat messages in there. So this is one of the good things about me. So is that it can handle the state for you. So you can see if I come in here and I say hi there. Sure enough, we get hi. I'm Isabella. Not nice to meet you. How can I help you today? Let me try and test it. All right. So let's say I tell it, okay, I'm Sam. My favorite color is blue. Let's see if we can get that. Okay. So it says nice to meet you. Sam blue is a lovely color. What's on your mind today? Okay. Let me ask it something about, tell me about the company that made you. All right. It's basically, I mean, a assistant. So I don't have a specific company that made me. It's interesting that it's not saying meta here. I was created through a process of machine learning and natural language processing. All right. Now you can see we've gone past this. So you can see now if I ask it something like what is my name? It responds. Now remember we asked for a race to sink answers back. So it's obviously getting that from the memory in the conversation. Right. If I ask it, what was my favorite color? Blue. Okay. So we can see that now we've basically got a chatbot going. We've got this going now. If we wanted to log these out somewhere, we could just add this in here. That you know, here where I'm doing this print part, we could actually just put in something to call another function to do a logging function to save it to a database to do a whole bunch of different things. It's very simple to get something going really quickly. So you can see in just this small amount of code, we've got a UI going for our chatbot that we've got going here. So if we wanted to test out a rag and we wanted to be able to do some things, maybe in the future, what I look at doing is we could have a chat going where we can actually see the rag outputs on the side and stuff like that as well. So there's a whole bunch of different ideas that you can do with this as you go through them. So hopefully this gives you a taste of what you could do and how you could get started with this. I'll probably use this to show some sort of local bots and stuff like that running with a Lama in the future. I do feel this is a nice simple way that you can get a UI going that you can get something that you can start testing. And you could actually then put this on something that you run in the cloud quite easily as well. So that other people could test it and you could see the results there. All right. Any comments or questions, please put them in the comments below. If you found the video useful, please click like and subscribe. And I will talk to you in the next video. Bye for now.\n"
          ]
        }
      ],
      "source": [
        "llm = OpenAI(model='text-davinci-003', temperature=0)\n",
        "embed_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating summary from transcribed file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Km7IajfIi5f"
      },
      "outputs": [],
      "source": [
        "\n",
        "with open(\"transcibed_file.txt\",\"r\") as file:\n",
        "  text = file.read()\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20, separators = [\" \", \",\", \"\\n\"])\n",
        "texts = text_splitter.split_text(text)\n",
        "docs = [Document(page_content=t) for t in text]\n",
        "prompt_template = \"\"\"Find this video transcript:\n",
        "{text}\n",
        "Create summary of this video in bullet point\\n\"\"\"\n",
        "\n",
        "summary_prompt = PromptTemplate(template=prompt_template,\n",
        "                        input_variables=[\"text\"])\n",
        "chain = load_summarize_chain(llm=llm, chain_type=\"stuff\",prompt=summary_prompt)\n",
        "output_summary = chain.run(docs)\n",
        "wrapped_text = textwrap.fill(output_summary,\n",
        "                             width=1000,\n",
        "                             break_long_words=False,\n",
        "                             replace_whitespace=False)\n",
        "print(wrapped_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gK4sJjl8mPl1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
